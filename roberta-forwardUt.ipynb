{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "2022-04-22 16:59:03.991600: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-22 16:59:03.991640: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/phd-fan.weiquan2/works/MELD-Baseline/wandb/run-20220422_165847-6v2qnn2d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/qftie/meld-emo-cls/runs/6v2qnn2d\" target=\"_blank\">dark-paper-9</a></strong> to <a href=\"https://wandb.ai/qftie/meld-emo-cls\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inside my model training code\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"meld-emo-cls\",entity='qftie',group='bert')\n",
    "\n",
    "config = wandb.config          # Initialize config\n",
    "config.batch_size = 8          # input batch size for training (default: 64)\n",
    "config.test_batch_size = 64    # input batch size for testing (default: 1000)\n",
    "config.epochs = 6             # number of epochs to train (default: 10)\n",
    "config.lr = 2e-5               # learning rate (default: 0.01)\n",
    "config.momentum = 0.1          # SGD momentum (default: 0.5) \n",
    "config.no_cuda = False         # disables CUDA training\n",
    "config.bert_path = 'roberta-base'\n",
    "config.exam_name = 'roberta-forwardUt'\n",
    "config.max_seq_len = 400\n",
    "config.max_turn = 4\n",
    "config.seed = 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 16:59:10.540148: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-22 16:59:10.540181: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json, time\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertConfig, AutoTokenizer, AdamW, get_cosine_schedule_with_warmup, AutoModel, AutoConfig\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "bert_path = config.bert_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_path, truncation_side=\"left\")   # 初始化分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seed everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo2id = {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger': 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备forward utterance作为sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back_stride = int(data_text_valid['Utterance_ID'][50][-3:])\n",
    "# ' '.join(data_text_valid['Utterance'][ 50- back_stride: 50].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_valid_forward_utt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meld_and_tokenize(file_path, train=False, context_turn = config.max_turn):\n",
    "    data_text = pd.read_csv(file_path)\n",
    "    data_text['Speaker_Utterance'] = data_text['Speaker'] + ':' + data_text['Utterance']\n",
    "    x= (data_text['Speaker_Utterance']).tolist()\n",
    "    label = [emo2id[x] for x in data_text['Emotion']]\n",
    "    context = []\n",
    "    for i in range(len(data_text)):\n",
    "        if data_text['Utterance_ID'][i]<context_turn:\n",
    "            back_stride = data_text['Utterance_ID'][i]\n",
    "        else:\n",
    "            back_stride = context_turn\n",
    "        utt_forward = ' '.join(data_text['Speaker_Utterance'][ i- back_stride: i].tolist())\n",
    "        context.append(utt_forward)\n",
    "    for i in range(len(x)):\n",
    "        x[i] = context[i] + '[SEP]' + x[i]\n",
    "    return tokenizer(x, truncation=True, padding=True, max_length=512),label\n",
    "    \n",
    "train_encoding, train_label = load_meld_and_tokenize(file_path='/148Dataset/data-tie.qianfeng/MELD/data/MELD/train_sent_emo.csv')\n",
    "valid_encoding, valid_label = load_meld_and_tokenize(file_path='/148Dataset/data-tie.qianfeng/MELD/data/MELD/dev_sent_emo.csv')\n",
    "test_encoding, test_label = load_meld_and_tokenize(file_path='/148Dataset/data-tie.qianfeng/MELD/data/MELD/test_sent_emo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 10975,\n",
       " 3388,\n",
       " 510,\n",
       " 742,\n",
       " 4771,\n",
       " 463,\n",
       " 1371,\n",
       " 35,\n",
       " 19726,\n",
       " 38,\n",
       " 21,\n",
       " 5,\n",
       " 477,\n",
       " 621,\n",
       " 15,\n",
       " 127,\n",
       " 138,\n",
       " 17,\n",
       " 27,\n",
       " 29,\n",
       " 3868,\n",
       " 31,\n",
       " 5,\n",
       " 26544,\n",
       " 12,\n",
       " 245,\n",
       " 7,\n",
       " 8837,\n",
       " 12,\n",
       " 401,\n",
       " 467,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoding['input_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备训练集，验证集，测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集读取 转成dict形式\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    # 读取单个样本\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsDataset(train_encoding, train_label)\n",
    "valid_dataset = NewsDataset(valid_encoding, valid_label)\n",
    "test_dataset = NewsDataset(test_encoding, test_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载到torch的dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单个读取到批量读取\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config.batch_size, sampler=sampler)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.test_batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义bert模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义model\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self, bert_path, classes=7):\n",
    "        super(Bert_Model, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(bert_path)  # 导入模型超参数\n",
    "        self.bert = AutoModel.from_pretrained(bert_path)     # 加载预训练模型权重\n",
    "        self.fc = nn.Linear(self.config.hidden_size, classes)  # 直接分类\n",
    "        self.dense = nn.Linear(self.config.hidden_size, self.config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.pred = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, 64),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, classes)\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, output_hidden_states= True, return_dict=True)\n",
    "        # out_pool = outputs[1]   # 池化后的输出 [bs, config.hidden_size]\n",
    "        out_pool = torch.mean(outputs.last_hidden_state, 1)\n",
    "        # out_pool = self.dense(out_pool)\n",
    "        # out_pool = self.activation(out_pool)\n",
    "        logit = self.pred(out_pool)   #  [bs, classes]\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实例化bert模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 125291278, Trainable parameters: 125291278\n"
     ]
    }
   ],
   "source": [
    "def get_parameter_number(model):\n",
    "    #  打印模型参数量\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = config.epochs\n",
    "model = Bert_Model(bert_path)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()\n",
    "print(get_parameter_number(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=1e-4) #AdamW优化器\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0.3*len(train_loader),\n",
    "                                            num_training_steps=EPOCHS*len(train_loader))\n",
    "# 学习率先线性warmup一个epoch，然后cosine式下降。\n",
    "# 这里给个小提示，一定要加warmup（学习率从0慢慢升上去），如果把warmup去掉，可能收敛不了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 准备放入多卡环境\n",
    "# model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义训练函数和验证测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型性能，在验证集上\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    val_true, val_pred = [], []\n",
    "    valid_loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(data_loader):\n",
    "            y_pred = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "            loss = criterion(y_pred, batch['labels'].to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "            val_true.extend(batch['labels'].cpu().numpy().tolist())\n",
    "            valid_loss_sum += loss.item()\n",
    "            \n",
    "    print(classification_report(val_true, val_pred, digits=4))\n",
    "    return accuracy_score(val_true, val_pred), valid_loss_sum/len(data_loader), f1_score(val_true, val_pred, average='macro')  #返回accuracy, loss, f1-macro\n",
    "\n",
    "\n",
    "# 测试集没有标签，需要预测提交\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            y_pred = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "    return val_pred\n",
    "\n",
    "\n",
    "def train_and_eval(model, train_loader, valid_loader, \n",
    "                   optimizer, scheduler, device, epoch):\n",
    "    best_acc = 0.0\n",
    "    patience = 0\n",
    "    best_loss = 100\n",
    "    best_macro_f1 = 0\n",
    "    b = 0.6\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for i in range(epoch):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
    "        train_loss_sum = 0.0\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            att = batch['attention_mask'].to(device)\n",
    "            y = batch['labels'].to(device)  \n",
    "            y_pred = model(ids, att)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss = (loss - b).abs() + b # This is it!\n",
    "            step_lr = np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()   # 学习率变化\n",
    "            \n",
    "            train_loss_sum += loss.item()\n",
    "            if (idx + 1) % (len(train_loader)//20) == 0:    # 只打印五次结果\n",
    "                wandb.log({\n",
    "                            'Epoch': i+1, \n",
    "                            'train_loss': loss,\n",
    "                            'lr': step_lr\n",
    "                            })\n",
    "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
    "                # print(\"Learning rate = {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        model.eval()\n",
    "        acc, valid_loss, valid_macro_f1 = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
    "        wandb.log({'valid_acc': acc, 'valid_loss': valid_loss})\n",
    "        # 保存最优模型\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            \n",
    "\n",
    "        if valid_macro_f1 > best_macro_f1:\n",
    "            best_macro_f1 = valid_macro_f1\n",
    "            torch.save(model.state_dict(), 'pytorch_model.bin') \n",
    "            \n",
    "        \n",
    "        print(\"current macro_f1 is {:.4f}, best macro_f1 is {:.4f}\".format(valid_macro_f1, best_macro_f1))\n",
    "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练和验证模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training epoch 1 *****\n",
      "Epoch 0001 | Step 0062/1249 | Loss 1.8102 | Time 7.5361\n",
      "Epoch 0001 | Step 0124/1249 | Loss 1.6821 | Time 14.2847\n",
      "Epoch 0001 | Step 0186/1249 | Loss 1.6213 | Time 21.0937\n",
      "Epoch 0001 | Step 0248/1249 | Loss 1.5718 | Time 27.9596\n",
      "Epoch 0001 | Step 0310/1249 | Loss 1.5353 | Time 34.9044\n",
      "Epoch 0001 | Step 0372/1249 | Loss 1.5057 | Time 42.2575\n",
      "Epoch 0001 | Step 0434/1249 | Loss 1.4843 | Time 49.5967\n",
      "Epoch 0001 | Step 0496/1249 | Loss 1.4614 | Time 56.9776\n",
      "Epoch 0001 | Step 0558/1249 | Loss 1.4457 | Time 64.0088\n",
      "Epoch 0001 | Step 0620/1249 | Loss 1.4246 | Time 70.9422\n",
      "Epoch 0001 | Step 0682/1249 | Loss 1.4057 | Time 77.8246\n",
      "Epoch 0001 | Step 0744/1249 | Loss 1.3946 | Time 84.7977\n",
      "Epoch 0001 | Step 0806/1249 | Loss 1.3891 | Time 91.7821\n",
      "Epoch 0001 | Step 0868/1249 | Loss 1.3774 | Time 98.6883\n",
      "Epoch 0001 | Step 0930/1249 | Loss 1.3671 | Time 105.5869\n",
      "Epoch 0001 | Step 0992/1249 | Loss 1.3609 | Time 112.5378\n",
      "Epoch 0001 | Step 1054/1249 | Loss 1.3455 | Time 119.4837\n",
      "Epoch 0001 | Step 1116/1249 | Loss 1.3406 | Time 126.4043\n",
      "Epoch 0001 | Step 1178/1249 | Loss 1.3351 | Time 133.3593\n",
      "Epoch 0001 | Step 1240/1249 | Loss 1.3263 | Time 140.2682\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6672    0.8915    0.7632       470\n",
      "           1     0.6514    0.4733    0.5483       150\n",
      "           2     0.0000    0.0000    0.0000        40\n",
      "           3     0.5238    0.1982    0.2876       111\n",
      "           4     0.5181    0.6135    0.5618       163\n",
      "           5     0.0000    0.0000    0.0000        22\n",
      "           6     0.3942    0.3529    0.3724       153\n",
      "\n",
      "    accuracy                         0.6005      1109\n",
      "   macro avg     0.3935    0.3614    0.3619      1109\n",
      "weighted avg     0.5538    0.6005    0.5603      1109\n",
      "\n",
      "current macro_f1 is 0.3619, best macro_f1 is 0.3619\n",
      "time costed = 148.5253s \n",
      "\n",
      "***** Running training epoch 2 *****\n",
      "Epoch 0002 | Step 0062/1249 | Loss 1.0647 | Time 6.9589\n",
      "Epoch 0002 | Step 0124/1249 | Loss 1.0661 | Time 13.8591\n",
      "Epoch 0002 | Step 0186/1249 | Loss 1.0754 | Time 20.7733\n",
      "Epoch 0002 | Step 0248/1249 | Loss 1.0743 | Time 27.7117\n",
      "Epoch 0002 | Step 0310/1249 | Loss 1.0823 | Time 34.6360\n",
      "Epoch 0002 | Step 0372/1249 | Loss 1.0799 | Time 41.5511\n",
      "Epoch 0002 | Step 0434/1249 | Loss 1.0809 | Time 48.5548\n",
      "Epoch 0002 | Step 0496/1249 | Loss 1.0797 | Time 56.2092\n",
      "Epoch 0002 | Step 0558/1249 | Loss 1.0748 | Time 63.4181\n",
      "Epoch 0002 | Step 0620/1249 | Loss 1.0735 | Time 70.4208\n",
      "Epoch 0002 | Step 0682/1249 | Loss 1.0734 | Time 77.5609\n",
      "Epoch 0002 | Step 0744/1249 | Loss 1.0715 | Time 84.8959\n",
      "Epoch 0002 | Step 0806/1249 | Loss 1.0707 | Time 92.2549\n",
      "Epoch 0002 | Step 0868/1249 | Loss 1.0698 | Time 99.5734\n",
      "Epoch 0002 | Step 0930/1249 | Loss 1.0747 | Time 106.9213\n",
      "Epoch 0002 | Step 0992/1249 | Loss 1.0741 | Time 114.2747\n",
      "Epoch 0002 | Step 1054/1249 | Loss 1.0797 | Time 121.0968\n",
      "Epoch 0002 | Step 1116/1249 | Loss 1.0809 | Time 127.9285\n",
      "Epoch 0002 | Step 1178/1249 | Loss 1.0808 | Time 134.7592\n",
      "Epoch 0002 | Step 1240/1249 | Loss 1.0815 | Time 141.5831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6853    0.8106    0.7427       470\n",
      "           1     0.4805    0.7400    0.5827       150\n",
      "           2     0.4286    0.0750    0.1277        40\n",
      "           3     0.5429    0.1712    0.2603       111\n",
      "           4     0.6279    0.4969    0.5548       163\n",
      "           5     0.2500    0.0455    0.0769        22\n",
      "           6     0.4354    0.4183    0.4267       153\n",
      "\n",
      "    accuracy                         0.5951      1109\n",
      "   macro avg     0.4929    0.3939    0.3960      1109\n",
      "weighted avg     0.5825    0.5951    0.5662      1109\n",
      "\n",
      "current macro_f1 is 0.3960, best macro_f1 is 0.3960\n",
      "time costed = 149.54216s \n",
      "\n",
      "***** Running training epoch 3 *****\n",
      "Epoch 0003 | Step 0062/1249 | Loss 0.9486 | Time 6.9374\n",
      "Epoch 0003 | Step 0124/1249 | Loss 0.9521 | Time 13.8930\n",
      "Epoch 0003 | Step 0186/1249 | Loss 0.9553 | Time 20.8781\n",
      "Epoch 0003 | Step 0248/1249 | Loss 0.9494 | Time 27.7113\n",
      "Epoch 0003 | Step 0310/1249 | Loss 0.9481 | Time 34.6847\n",
      "Epoch 0003 | Step 0372/1249 | Loss 0.9462 | Time 41.8120\n",
      "Epoch 0003 | Step 0434/1249 | Loss 0.9506 | Time 48.7503\n",
      "Epoch 0003 | Step 0496/1249 | Loss 0.9476 | Time 55.7565\n",
      "Epoch 0003 | Step 0558/1249 | Loss 0.9444 | Time 62.7171\n",
      "Epoch 0003 | Step 0620/1249 | Loss 0.9406 | Time 69.7815\n",
      "Epoch 0003 | Step 0682/1249 | Loss 0.9476 | Time 76.6723\n",
      "Epoch 0003 | Step 0744/1249 | Loss 0.9506 | Time 83.5819\n",
      "Epoch 0003 | Step 0806/1249 | Loss 0.9545 | Time 90.4660\n",
      "Epoch 0003 | Step 0868/1249 | Loss 0.9545 | Time 97.3626\n",
      "Epoch 0003 | Step 0930/1249 | Loss 0.9534 | Time 104.4454\n",
      "Epoch 0003 | Step 0992/1249 | Loss 0.9560 | Time 111.3464\n",
      "Epoch 0003 | Step 1054/1249 | Loss 0.9539 | Time 118.2345\n",
      "Epoch 0003 | Step 1116/1249 | Loss 0.9533 | Time 125.3083\n",
      "Epoch 0003 | Step 1178/1249 | Loss 0.9535 | Time 132.3787\n",
      "Epoch 0003 | Step 1240/1249 | Loss 0.9549 | Time 139.2787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6715    0.8957    0.7675       470\n",
      "           1     0.6667    0.5733    0.6165       150\n",
      "           2     0.4211    0.2000    0.2712        40\n",
      "           3     0.5882    0.1802    0.2759       111\n",
      "           4     0.5475    0.6012    0.5731       163\n",
      "           5     0.2222    0.0909    0.1290        22\n",
      "           6     0.4821    0.3529    0.4075       153\n",
      "\n",
      "    accuracy                         0.6213      1109\n",
      "   macro avg     0.5142    0.4135    0.4344      1109\n",
      "weighted avg     0.6002    0.6213    0.5891      1109\n",
      "\n",
      "current macro_f1 is 0.4344, best macro_f1 is 0.4344\n",
      "time costed = 147.34633s \n",
      "\n",
      "***** Running training epoch 4 *****\n",
      "Epoch 0004 | Step 0062/1249 | Loss 0.8488 | Time 6.9024\n",
      "Epoch 0004 | Step 0124/1249 | Loss 0.8299 | Time 13.8223\n",
      "Epoch 0004 | Step 0186/1249 | Loss 0.8565 | Time 20.7143\n",
      "Epoch 0004 | Step 0248/1249 | Loss 0.8506 | Time 28.0018\n",
      "Epoch 0004 | Step 0310/1249 | Loss 0.8469 | Time 35.0131\n",
      "Epoch 0004 | Step 0372/1249 | Loss 0.8508 | Time 41.9405\n",
      "Epoch 0004 | Step 0434/1249 | Loss 0.8472 | Time 48.8509\n",
      "Epoch 0004 | Step 0496/1249 | Loss 0.8526 | Time 55.7372\n",
      "Epoch 0004 | Step 0558/1249 | Loss 0.8548 | Time 62.6542\n",
      "Epoch 0004 | Step 0620/1249 | Loss 0.8542 | Time 69.5876\n",
      "Epoch 0004 | Step 0682/1249 | Loss 0.8534 | Time 76.5334\n",
      "Epoch 0004 | Step 0744/1249 | Loss 0.8516 | Time 83.4226\n",
      "Epoch 0004 | Step 0806/1249 | Loss 0.8542 | Time 90.3118\n",
      "Epoch 0004 | Step 0868/1249 | Loss 0.8549 | Time 97.2009\n",
      "Epoch 0004 | Step 0930/1249 | Loss 0.8552 | Time 104.0622\n",
      "Epoch 0004 | Step 0992/1249 | Loss 0.8580 | Time 111.0065\n",
      "Epoch 0004 | Step 1054/1249 | Loss 0.8598 | Time 117.9516\n",
      "Epoch 0004 | Step 1116/1249 | Loss 0.8602 | Time 124.8664\n",
      "Epoch 0004 | Step 1178/1249 | Loss 0.8566 | Time 131.7594\n",
      "Epoch 0004 | Step 1240/1249 | Loss 0.8576 | Time 138.6923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7024    0.8085    0.7517       470\n",
      "           1     0.5860    0.7267    0.6488       150\n",
      "           2     0.3824    0.3250    0.3514        40\n",
      "           3     0.4706    0.2883    0.3575       111\n",
      "           4     0.5156    0.6074    0.5577       163\n",
      "           5     0.2500    0.2273    0.2381        22\n",
      "           6     0.5735    0.2549    0.3529       153\n",
      "\n",
      "    accuracy                         0.6105      1109\n",
      "   macro avg     0.4972    0.4626    0.4655      1109\n",
      "weighted avg     0.5977    0.6105    0.5902      1109\n",
      "\n",
      "current macro_f1 is 0.4655, best macro_f1 is 0.4655\n",
      "time costed = 146.80215s \n",
      "\n",
      "***** Running training epoch 5 *****\n",
      "Epoch 0005 | Step 0062/1249 | Loss 0.8320 | Time 7.0085\n",
      "Epoch 0005 | Step 0124/1249 | Loss 0.8198 | Time 13.9081\n",
      "Epoch 0005 | Step 0186/1249 | Loss 0.8223 | Time 20.7668\n",
      "Epoch 0005 | Step 0248/1249 | Loss 0.8149 | Time 27.6892\n",
      "Epoch 0005 | Step 0310/1249 | Loss 0.8128 | Time 34.6208\n",
      "Epoch 0005 | Step 0372/1249 | Loss 0.8099 | Time 41.5339\n",
      "Epoch 0005 | Step 0434/1249 | Loss 0.8107 | Time 48.4042\n",
      "Epoch 0005 | Step 0496/1249 | Loss 0.8161 | Time 55.2757\n",
      "Epoch 0005 | Step 0558/1249 | Loss 0.8185 | Time 62.5088\n",
      "Epoch 0005 | Step 0620/1249 | Loss 0.8217 | Time 69.4570\n",
      "Epoch 0005 | Step 0682/1249 | Loss 0.8174 | Time 76.3926\n",
      "Epoch 0005 | Step 0744/1249 | Loss 0.8119 | Time 83.2475\n",
      "Epoch 0005 | Step 0806/1249 | Loss 0.8120 | Time 90.0768\n",
      "Epoch 0005 | Step 0868/1249 | Loss 0.8123 | Time 96.9068\n",
      "Epoch 0005 | Step 0930/1249 | Loss 0.8116 | Time 103.7306\n",
      "Epoch 0005 | Step 0992/1249 | Loss 0.8101 | Time 110.5583\n",
      "Epoch 0005 | Step 1054/1249 | Loss 0.8090 | Time 117.3844\n",
      "Epoch 0005 | Step 1116/1249 | Loss 0.8096 | Time 124.2200\n",
      "Epoch 0005 | Step 1178/1249 | Loss 0.8075 | Time 131.0503\n",
      "Epoch 0005 | Step 1240/1249 | Loss 0.8061 | Time 137.8742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7013    0.8340    0.7619       470\n",
      "           1     0.6114    0.7133    0.6585       150\n",
      "           2     0.3448    0.2500    0.2899        40\n",
      "           3     0.5085    0.2703    0.3529       111\n",
      "           4     0.5486    0.5890    0.5680       163\n",
      "           5     0.3182    0.3182    0.3182        22\n",
      "           6     0.5222    0.3072    0.3868       153\n",
      "\n",
      "    accuracy                         0.6213      1109\n",
      "   macro avg     0.5079    0.4689    0.4766      1109\n",
      "weighted avg     0.6022    0.6213    0.6009      1109\n",
      "\n",
      "current macro_f1 is 0.4766, best macro_f1 is 0.4766\n",
      "time costed = 145.94959s \n",
      "\n",
      "***** Running training epoch 6 *****\n",
      "Epoch 0006 | Step 0062/1249 | Loss 0.7610 | Time 7.0049\n",
      "Epoch 0006 | Step 0124/1249 | Loss 0.7751 | Time 13.9983\n",
      "Epoch 0006 | Step 0186/1249 | Loss 0.7726 | Time 21.0079\n",
      "Epoch 0006 | Step 0248/1249 | Loss 0.7672 | Time 28.0781\n",
      "Epoch 0006 | Step 0310/1249 | Loss 0.7734 | Time 35.0858\n",
      "Epoch 0006 | Step 0372/1249 | Loss 0.7731 | Time 42.1822\n",
      "Epoch 0006 | Step 0434/1249 | Loss 0.7722 | Time 49.1782\n",
      "Epoch 0006 | Step 0496/1249 | Loss 0.7726 | Time 55.9986\n",
      "Epoch 0006 | Step 0558/1249 | Loss 0.7734 | Time 62.8251\n",
      "Epoch 0006 | Step 0620/1249 | Loss 0.7721 | Time 69.6446\n",
      "Epoch 0006 | Step 0682/1249 | Loss 0.7726 | Time 76.4705\n",
      "Epoch 0006 | Step 0744/1249 | Loss 0.7723 | Time 83.2957\n",
      "Epoch 0006 | Step 0806/1249 | Loss 0.7745 | Time 90.1174\n",
      "Epoch 0006 | Step 0868/1249 | Loss 0.7737 | Time 96.9415\n",
      "Epoch 0006 | Step 0930/1249 | Loss 0.7740 | Time 103.8376\n",
      "Epoch 0006 | Step 0992/1249 | Loss 0.7730 | Time 110.8558\n",
      "Epoch 0006 | Step 1054/1249 | Loss 0.7737 | Time 117.7885\n",
      "Epoch 0006 | Step 1116/1249 | Loss 0.7738 | Time 124.8712\n",
      "Epoch 0006 | Step 1178/1249 | Loss 0.7733 | Time 131.8176\n",
      "Epoch 0006 | Step 1240/1249 | Loss 0.7737 | Time 138.7937\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7146    0.8149    0.7614       470\n",
      "           1     0.6471    0.6600    0.6535       150\n",
      "           2     0.3143    0.2750    0.2933        40\n",
      "           3     0.4375    0.3153    0.3665       111\n",
      "           4     0.5304    0.5890    0.5581       163\n",
      "           5     0.3043    0.3182    0.3111        22\n",
      "           6     0.5050    0.3333    0.4016       153\n",
      "\n",
      "    accuracy                         0.6150      1109\n",
      "   macro avg     0.4933    0.4722    0.4779      1109\n",
      "weighted avg     0.5991    0.6150    0.6020      1109\n",
      "\n",
      "current macro_f1 is 0.4779, best macro_f1 is 0.4779\n",
      "time costed = 146.55454s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练和验证评估\n",
    "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载最优模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7559    0.8185    0.7859      1256\n",
      "           1     0.5151    0.5480    0.5310       281\n",
      "           2     0.2800    0.2800    0.2800        50\n",
      "           3     0.4400    0.3173    0.3687       208\n",
      "           4     0.6053    0.6219    0.6135       402\n",
      "           5     0.2917    0.2059    0.2414        68\n",
      "           6     0.5310    0.4464    0.4850       345\n",
      "\n",
      "    accuracy                         0.6437      2610\n",
      "   macro avg     0.4884    0.4626    0.4722      2610\n",
      "weighted avg     0.6307    0.6437    0.6350      2610\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6febb15e45124dba9ddb63dcf25ab407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▂▄▄▄▄▄▄▅▅▅▅▅▅▅▇▇▇▇▇▇███████</td></tr><tr><td>lr</td><td>▂▆███████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>▇▃▇▁▂▃▄▂▂▅▅█▆▃▂▁▂▅▁▂▃▂▆▂▂▃▂▁▂▂▂▁▁▃▂▁▃▂▁▂</td></tr><tr><td>valid_acc</td><td>▂▁█▅█▆</td></tr><tr><td>valid_loss</td><td>█▆▃▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>6</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.84557</td></tr><tr><td>valid_acc</td><td>0.61497</td></tr><tr><td>valid_loss</td><td>1.11149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dark-paper-9</strong>: <a href=\"https://wandb.ai/qftie/meld-emo-cls/runs/6v2qnn2d\" target=\"_blank\">https://wandb.ai/qftie/meld-emo-cls/runs/6v2qnn2d</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220422_165847-6v2qnn2d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载最优权重对测试集测试\n",
    "model.load_state_dict(torch.load(\"pytorch_model.bin\"))\n",
    "pred_test = evaluate(model, test_loader, DEVICE)\n",
    "# print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(test_label, pred_test)))\n",
    "# print(classification_report(test_label, pred_test, digits=4))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = 'output/ch-roberta-dorwardUt'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# torch.save(model.state_dict(), output_dir+\"/pytorch_model.bin\")\n",
    "# # torch.save(model, \"pytorch_model_whole.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49e5bd26eb5fb0a8ffb649f62262c127e261ba51230dc2578599ab5938abf7ca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('torch17py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

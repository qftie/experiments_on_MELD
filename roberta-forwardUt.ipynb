{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqftie\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 09:50:57.799716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-25 09:50:57.799750: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/phd-fan.weiquan2/works/experiments_on_MELD/wandb/run-20220425_095055-3qu2w28o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/qftie/meld-emo-cls/runs/3qu2w28o\" target=\"_blank\">classic-voice-11</a></strong> to <a href=\"https://wandb.ai/qftie/meld-emo-cls\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inside my model training code\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"meld-emo-cls\",entity='qftie',group='bert')\n",
    "\n",
    "config = wandb.config          # Initialize config\n",
    "config.batch_size = 8          # input batch size for training (default: 64)\n",
    "config.test_batch_size = 64    # input batch size for testing (default: 1000)\n",
    "config.epochs = 6             # number of epochs to train (default: 10)\n",
    "config.lr = 2e-5               # learning rate (default: 0.01)\n",
    "config.momentum = 0.1          # SGD momentum (default: 0.5) \n",
    "config.no_cuda = False         # disables CUDA training\n",
    "config.bert_path = 'roberta-base'\n",
    "config.exam_name = 'roberta-forwardUt'\n",
    "config.max_seq_len = 400\n",
    "config.max_turn = 0\n",
    "config.seed = 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json, time\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertConfig, AutoTokenizer, AdamW, get_cosine_schedule_with_warmup, AutoModel, AutoConfig\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "bert_path = config.bert_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_path, truncation_side=\"left\")   # 初始化分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seed everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo2id = {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger': 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备forward utterance作为sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back_stride = int(data_text_valid['Utterance_ID'][50][-3:])\n",
    "# ' '.join(data_text_valid['Utterance'][ 50- back_stride: 50].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_valid_forward_utt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meld_and_tokenize(file_path, train=False, context_turn = config.max_turn):\n",
    "    data_text = pd.read_csv(file_path)\n",
    "    data_text['Speaker_Utterance'] = data_text['Speaker'] + ':' + data_text['Utterance']\n",
    "    x= (data_text['Speaker_Utterance']).tolist()\n",
    "    label = [emo2id[x] for x in data_text['Emotion']]\n",
    "    context = []\n",
    "    for i in range(len(data_text)):\n",
    "        if data_text['Utterance_ID'][i]<context_turn:\n",
    "            back_stride = data_text['Utterance_ID'][i]\n",
    "        else:\n",
    "            back_stride = context_turn\n",
    "        utt_forward = ' '.join(data_text['Speaker_Utterance'][ i- back_stride: i].tolist())\n",
    "        context.append(utt_forward)\n",
    "    for i in range(len(x)):\n",
    "        x[i] = context[i] + '[SEP]' + x[i]\n",
    "    return tokenizer(x, truncation=True, padding=True, max_length=512),label\n",
    "    \n",
    "train_encoding, train_label = load_meld_and_tokenize(file_path='/148Dataset/data-tie.qianfeng/MELD/data/MELD/train_sent_emo.csv')\n",
    "valid_encoding, valid_label = load_meld_and_tokenize(file_path='/148Dataset/data-tie.qianfeng/MELD/data/MELD/dev_sent_emo.csv')\n",
    "test_encoding, test_label = load_meld_and_tokenize(file_path='/148Dataset/data-tie.qianfeng/MELD/data/MELD/test_sent_emo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 10975,\n",
       " 3388,\n",
       " 510,\n",
       " 742,\n",
       " 4771,\n",
       " 463,\n",
       " 1371,\n",
       " 35,\n",
       " 19726,\n",
       " 38,\n",
       " 21,\n",
       " 5,\n",
       " 477,\n",
       " 621,\n",
       " 15,\n",
       " 127,\n",
       " 138,\n",
       " 17,\n",
       " 27,\n",
       " 29,\n",
       " 3868,\n",
       " 31,\n",
       " 5,\n",
       " 26544,\n",
       " 12,\n",
       " 245,\n",
       " 7,\n",
       " 8837,\n",
       " 12,\n",
       " 401,\n",
       " 467,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoding['input_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备训练集，验证集，测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集读取 转成dict形式\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    # 读取单个样本\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsDataset(train_encoding, train_label)\n",
    "valid_dataset = NewsDataset(valid_encoding, valid_label)\n",
    "test_dataset = NewsDataset(test_encoding, test_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载到torch的dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单个读取到批量读取\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config.batch_size, sampler=sampler)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.test_batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义bert模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义model\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self, bert_path, classes=7):\n",
    "        super(Bert_Model, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(bert_path)  # 导入模型超参数\n",
    "        self.bert = AutoModel.from_pretrained(bert_path)     # 加载预训练模型权重\n",
    "        self.fc = nn.Linear(self.config.hidden_size, classes)  # 直接分类\n",
    "        self.dense = nn.Linear(self.config.hidden_size, self.config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.pred = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, 64),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, classes)\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, output_hidden_states= True, return_dict=True)\n",
    "        # out_pool = outputs[1]   # 池化后的输出 [bs, config.hidden_size]\n",
    "        out_pool = torch.mean(outputs.last_hidden_state, 1)\n",
    "        # out_pool = self.dense(out_pool)\n",
    "        # out_pool = self.activation(out_pool)\n",
    "        logit = self.pred(out_pool)   #  [bs, classes]\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实例化bert模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 125291278, Trainable parameters: 125291278\n"
     ]
    }
   ],
   "source": [
    "def get_parameter_number(model):\n",
    "    #  打印模型参数量\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = config.epochs\n",
    "model = Bert_Model(bert_path)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()\n",
    "print(get_parameter_number(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=1e-4) #AdamW优化器\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0.3*len(train_loader),\n",
    "                                            num_training_steps=EPOCHS*len(train_loader))\n",
    "# 学习率先线性warmup一个epoch，然后cosine式下降。\n",
    "# 这里给个小提示，一定要加warmup（学习率从0慢慢升上去），如果把warmup去掉，可能收敛不了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 准备放入多卡环境\n",
    "# model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义训练函数和验证测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型性能，在验证集上\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    val_true, val_pred = [], []\n",
    "    valid_loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(data_loader):\n",
    "            y_pred = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "            loss = criterion(y_pred, batch['labels'].to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "            val_true.extend(batch['labels'].cpu().numpy().tolist())\n",
    "            valid_loss_sum += loss.item()\n",
    "            \n",
    "    print(classification_report(val_true, val_pred, digits=4))\n",
    "    return accuracy_score(val_true, val_pred), valid_loss_sum/len(data_loader), f1_score(val_true, val_pred, average='macro')  #返回accuracy, loss, f1-macro\n",
    "\n",
    "\n",
    "# 测试集没有标签，需要预测提交\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            y_pred = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "    return val_pred\n",
    "\n",
    "\n",
    "def train_and_eval(model, train_loader, valid_loader, \n",
    "                   optimizer, scheduler, device, epoch):\n",
    "    best_acc = 0.0\n",
    "    patience = 0\n",
    "    best_loss = 100\n",
    "    best_macro_f1 = 0\n",
    "    b = 0.6\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for i in range(epoch):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
    "        train_loss_sum = 0.0\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            att = batch['attention_mask'].to(device)\n",
    "            y = batch['labels'].to(device)  \n",
    "            y_pred = model(ids, att)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss = (loss - b).abs() + b # This is it!\n",
    "            step_lr = np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()   # 学习率变化\n",
    "            \n",
    "            train_loss_sum += loss.item()\n",
    "            if (idx + 1) % (len(train_loader)//20) == 0:    # 只打印五次结果\n",
    "                wandb.log({\n",
    "                            'Epoch': i+1, \n",
    "                            'train_loss': loss,\n",
    "                            'lr': step_lr\n",
    "                            })\n",
    "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
    "                # print(\"Learning rate = {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        model.eval()\n",
    "        acc, valid_loss, valid_macro_f1 = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
    "        wandb.log({'valid_acc': acc, 'valid_loss': valid_loss})\n",
    "        # 保存最优模型\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            \n",
    "\n",
    "        if valid_macro_f1 > best_macro_f1:\n",
    "            best_macro_f1 = valid_macro_f1\n",
    "            torch.save(model.state_dict(), 'pytorch_model.bin') \n",
    "            \n",
    "        \n",
    "        print(\"current macro_f1 is {:.4f}, best macro_f1 is {:.4f}\".format(valid_macro_f1, best_macro_f1))\n",
    "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练和验证模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training epoch 1 *****\n",
      "Epoch 0001 | Step 0062/1249 | Loss 1.7949 | Time 4.6365\n",
      "Epoch 0001 | Step 0124/1249 | Loss 1.6651 | Time 9.3065\n",
      "Epoch 0001 | Step 0186/1249 | Loss 1.5853 | Time 13.9354\n",
      "Epoch 0001 | Step 0248/1249 | Loss 1.5268 | Time 18.6432\n",
      "Epoch 0001 | Step 0310/1249 | Loss 1.4769 | Time 23.3055\n",
      "Epoch 0001 | Step 0372/1249 | Loss 1.4559 | Time 27.9889\n",
      "Epoch 0001 | Step 0434/1249 | Loss 1.4359 | Time 32.6929\n",
      "Epoch 0001 | Step 0496/1249 | Loss 1.4100 | Time 37.4027\n",
      "Epoch 0001 | Step 0558/1249 | Loss 1.3935 | Time 42.4736\n",
      "Epoch 0001 | Step 0620/1249 | Loss 1.3685 | Time 47.8519\n",
      "Epoch 0001 | Step 0682/1249 | Loss 1.3482 | Time 52.9233\n",
      "Epoch 0001 | Step 0744/1249 | Loss 1.3350 | Time 57.8328\n",
      "Epoch 0001 | Step 0806/1249 | Loss 1.3334 | Time 62.6329\n",
      "Epoch 0001 | Step 0868/1249 | Loss 1.3240 | Time 67.3780\n",
      "Epoch 0001 | Step 0930/1249 | Loss 1.3159 | Time 72.1117\n",
      "Epoch 0001 | Step 0992/1249 | Loss 1.3089 | Time 76.7968\n",
      "Epoch 0001 | Step 1054/1249 | Loss 1.2983 | Time 81.6824\n",
      "Epoch 0001 | Step 1116/1249 | Loss 1.2954 | Time 86.4747\n",
      "Epoch 0001 | Step 1178/1249 | Loss 1.2921 | Time 91.1697\n",
      "Epoch 0001 | Step 1240/1249 | Loss 1.2849 | Time 95.9233\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6826    0.8830    0.7699       470\n",
      "           1     0.6116    0.4933    0.5461       150\n",
      "           2     0.0000    0.0000    0.0000        40\n",
      "           3     0.5306    0.2342    0.3250       111\n",
      "           4     0.5224    0.6442    0.5769       163\n",
      "           5     0.0000    0.0000    0.0000        22\n",
      "           6     0.4186    0.3529    0.3830       153\n",
      "\n",
      "    accuracy                         0.6078      1109\n",
      "   macro avg     0.3951    0.3725    0.3716      1109\n",
      "weighted avg     0.5596    0.6078    0.5703      1109\n",
      "\n",
      "current macro_f1 is 0.3716, best macro_f1 is 0.3716\n",
      "time costed = 101.98907s \n",
      "\n",
      "***** Running training epoch 2 *****\n",
      "Epoch 0002 | Step 0062/1249 | Loss 1.0745 | Time 5.5344\n",
      "Epoch 0002 | Step 0124/1249 | Loss 1.0804 | Time 10.9015\n",
      "Epoch 0002 | Step 0186/1249 | Loss 1.0790 | Time 16.2991\n",
      "Epoch 0002 | Step 0248/1249 | Loss 1.0663 | Time 21.6853\n",
      "Epoch 0002 | Step 0310/1249 | Loss 1.0765 | Time 27.0699\n",
      "Epoch 0002 | Step 0372/1249 | Loss 1.0766 | Time 31.8548\n",
      "Epoch 0002 | Step 0434/1249 | Loss 1.0772 | Time 36.5724\n",
      "Epoch 0002 | Step 0496/1249 | Loss 1.0783 | Time 41.3162\n",
      "Epoch 0002 | Step 0558/1249 | Loss 1.0715 | Time 46.5056\n",
      "Epoch 0002 | Step 0620/1249 | Loss 1.0715 | Time 51.8747\n",
      "Epoch 0002 | Step 0682/1249 | Loss 1.0710 | Time 57.2214\n",
      "Epoch 0002 | Step 0744/1249 | Loss 1.0730 | Time 62.6117\n",
      "Epoch 0002 | Step 0806/1249 | Loss 1.0713 | Time 67.9723\n",
      "Epoch 0002 | Step 0868/1249 | Loss 1.0680 | Time 73.3388\n",
      "Epoch 0002 | Step 0930/1249 | Loss 1.0721 | Time 78.7263\n",
      "Epoch 0002 | Step 0992/1249 | Loss 1.0731 | Time 83.9334\n",
      "Epoch 0002 | Step 1054/1249 | Loss 1.0768 | Time 88.7410\n",
      "Epoch 0002 | Step 1116/1249 | Loss 1.0803 | Time 93.5041\n",
      "Epoch 0002 | Step 1178/1249 | Loss 1.0803 | Time 98.2500\n",
      "Epoch 0002 | Step 1240/1249 | Loss 1.0809 | Time 103.0200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6767    0.8638    0.7589       470\n",
      "           1     0.5330    0.7000    0.6052       150\n",
      "           2     0.4545    0.1250    0.1961        40\n",
      "           3     0.5769    0.1351    0.2190       111\n",
      "           4     0.5706    0.5951    0.5826       163\n",
      "           5     0.5000    0.0909    0.1538        22\n",
      "           6     0.4950    0.3268    0.3937       153\n",
      "\n",
      "    accuracy                         0.6132      1109\n",
      "   macro avg     0.5438    0.4053    0.4156      1109\n",
      "weighted avg     0.5951    0.6132    0.5755      1109\n",
      "\n",
      "current macro_f1 is 0.4156, best macro_f1 is 0.4156\n",
      "time costed = 109.07051s \n",
      "\n",
      "***** Running training epoch 3 *****\n",
      "Epoch 0003 | Step 0062/1249 | Loss 0.9595 | Time 4.8915\n",
      "Epoch 0003 | Step 0124/1249 | Loss 0.9496 | Time 9.5766\n",
      "Epoch 0003 | Step 0186/1249 | Loss 0.9566 | Time 14.2829\n",
      "Epoch 0003 | Step 0248/1249 | Loss 0.9525 | Time 19.0097\n",
      "Epoch 0003 | Step 0310/1249 | Loss 0.9478 | Time 23.7124\n",
      "Epoch 0003 | Step 0372/1249 | Loss 0.9502 | Time 28.7161\n",
      "Epoch 0003 | Step 0434/1249 | Loss 0.9524 | Time 34.0381\n",
      "Epoch 0003 | Step 0496/1249 | Loss 0.9503 | Time 39.4095\n",
      "Epoch 0003 | Step 0558/1249 | Loss 0.9450 | Time 44.6229\n",
      "Epoch 0003 | Step 0620/1249 | Loss 0.9439 | Time 49.3306\n",
      "Epoch 0003 | Step 0682/1249 | Loss 0.9538 | Time 54.0971\n",
      "Epoch 0003 | Step 0744/1249 | Loss 0.9547 | Time 58.8249\n",
      "Epoch 0003 | Step 0806/1249 | Loss 0.9614 | Time 63.6733\n",
      "Epoch 0003 | Step 0868/1249 | Loss 0.9619 | Time 68.4345\n",
      "Epoch 0003 | Step 0930/1249 | Loss 0.9600 | Time 73.2030\n",
      "Epoch 0003 | Step 0992/1249 | Loss 0.9618 | Time 78.0088\n",
      "Epoch 0003 | Step 1054/1249 | Loss 0.9572 | Time 82.7748\n",
      "Epoch 0003 | Step 1116/1249 | Loss 0.9576 | Time 87.5902\n",
      "Epoch 0003 | Step 1178/1249 | Loss 0.9588 | Time 92.3719\n",
      "Epoch 0003 | Step 1240/1249 | Loss 0.9588 | Time 97.1350\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6767    0.8596    0.7573       470\n",
      "           1     0.5783    0.6400    0.6076       150\n",
      "           2     0.5333    0.2000    0.2909        40\n",
      "           3     0.5455    0.3243    0.4068       111\n",
      "           4     0.5644    0.5644    0.5644       163\n",
      "           5     0.4000    0.0909    0.1481        22\n",
      "           6     0.5258    0.3333    0.4080       153\n",
      "\n",
      "    accuracy                         0.6213      1109\n",
      "   macro avg     0.5463    0.4304    0.4547      1109\n",
      "weighted avg     0.6023    0.6213    0.5965      1109\n",
      "\n",
      "current macro_f1 is 0.4547, best macro_f1 is 0.4547\n",
      "time costed = 103.16979s \n",
      "\n",
      "***** Running training epoch 4 *****\n",
      "Epoch 0004 | Step 0062/1249 | Loss 0.9006 | Time 5.5280\n",
      "Epoch 0004 | Step 0124/1249 | Loss 0.8645 | Time 10.9388\n",
      "Epoch 0004 | Step 0186/1249 | Loss 0.8850 | Time 16.3251\n",
      "Epoch 0004 | Step 0248/1249 | Loss 0.8724 | Time 21.6924\n",
      "Epoch 0004 | Step 0310/1249 | Loss 0.8663 | Time 27.0664\n",
      "Epoch 0004 | Step 0372/1249 | Loss 0.8665 | Time 32.4593\n",
      "Epoch 0004 | Step 0434/1249 | Loss 0.8660 | Time 37.8190\n",
      "Epoch 0004 | Step 0496/1249 | Loss 0.8688 | Time 43.2010\n",
      "Epoch 0004 | Step 0558/1249 | Loss 0.8702 | Time 48.3780\n",
      "Epoch 0004 | Step 0620/1249 | Loss 0.8693 | Time 53.5613\n",
      "Epoch 0004 | Step 0682/1249 | Loss 0.8674 | Time 58.6335\n",
      "Epoch 0004 | Step 0744/1249 | Loss 0.8666 | Time 63.3804\n",
      "Epoch 0004 | Step 0806/1249 | Loss 0.8672 | Time 68.1451\n",
      "Epoch 0004 | Step 0868/1249 | Loss 0.8677 | Time 72.9730\n",
      "Epoch 0004 | Step 0930/1249 | Loss 0.8702 | Time 77.7765\n",
      "Epoch 0004 | Step 0992/1249 | Loss 0.8709 | Time 82.5649\n",
      "Epoch 0004 | Step 1054/1249 | Loss 0.8704 | Time 87.2852\n",
      "Epoch 0004 | Step 1116/1249 | Loss 0.8715 | Time 92.0452\n",
      "Epoch 0004 | Step 1178/1249 | Loss 0.8685 | Time 96.7796\n",
      "Epoch 0004 | Step 1240/1249 | Loss 0.8677 | Time 101.5185\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7026    0.8191    0.7564       470\n",
      "           1     0.5271    0.7133    0.6062       150\n",
      "           2     0.3750    0.1500    0.2143        40\n",
      "           3     0.4848    0.2883    0.3616       111\n",
      "           4     0.5140    0.5644    0.5380       163\n",
      "           5     0.2000    0.0909    0.1250        22\n",
      "           6     0.5057    0.2876    0.3667       153\n",
      "\n",
      "    accuracy                         0.6023      1109\n",
      "   macro avg     0.4727    0.4162    0.4240      1109\n",
      "weighted avg     0.5804    0.6023    0.5786      1109\n",
      "\n",
      "current macro_f1 is 0.4240, best macro_f1 is 0.4547\n",
      "time costed = 104.3295s \n",
      "\n",
      "***** Running training epoch 5 *****\n",
      "Epoch 0005 | Step 0062/1249 | Loss 0.8536 | Time 4.7873\n",
      "Epoch 0005 | Step 0124/1249 | Loss 0.8229 | Time 9.5255\n",
      "Epoch 0005 | Step 0186/1249 | Loss 0.8197 | Time 14.2646\n",
      "Epoch 0005 | Step 0248/1249 | Loss 0.8133 | Time 19.2253\n",
      "Epoch 0005 | Step 0310/1249 | Loss 0.8101 | Time 24.5268\n",
      "Epoch 0005 | Step 0372/1249 | Loss 0.8096 | Time 29.3214\n",
      "Epoch 0005 | Step 0434/1249 | Loss 0.8118 | Time 34.0437\n",
      "Epoch 0005 | Step 0496/1249 | Loss 0.8151 | Time 38.8478\n",
      "Epoch 0005 | Step 0558/1249 | Loss 0.8132 | Time 43.6463\n",
      "Epoch 0005 | Step 0620/1249 | Loss 0.8133 | Time 48.3577\n",
      "Epoch 0005 | Step 0682/1249 | Loss 0.8135 | Time 53.1229\n",
      "Epoch 0005 | Step 0744/1249 | Loss 0.8120 | Time 57.8892\n",
      "Epoch 0005 | Step 0806/1249 | Loss 0.8116 | Time 62.6743\n",
      "Epoch 0005 | Step 0868/1249 | Loss 0.8140 | Time 67.4437\n",
      "Epoch 0005 | Step 0930/1249 | Loss 0.8145 | Time 72.1729\n",
      "Epoch 0005 | Step 0992/1249 | Loss 0.8154 | Time 76.9211\n",
      "Epoch 0005 | Step 1054/1249 | Loss 0.8148 | Time 81.6533\n",
      "Epoch 0005 | Step 1116/1249 | Loss 0.8160 | Time 86.3806\n",
      "Epoch 0005 | Step 1178/1249 | Loss 0.8146 | Time 91.0984\n",
      "Epoch 0005 | Step 1240/1249 | Loss 0.8126 | Time 95.8358\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6906    0.8170    0.7485       470\n",
      "           1     0.5213    0.6533    0.5799       150\n",
      "           2     0.3913    0.2250    0.2857        40\n",
      "           3     0.4833    0.2613    0.3392       111\n",
      "           4     0.5145    0.5460    0.5298       163\n",
      "           5     0.2308    0.1364    0.1714        22\n",
      "           6     0.5104    0.3203    0.3936       153\n",
      "\n",
      "    accuracy                         0.5960      1109\n",
      "   macro avg     0.4775    0.4228    0.4354      1109\n",
      "weighted avg     0.5763    0.5960    0.5755      1109\n",
      "\n",
      "current macro_f1 is 0.4354, best macro_f1 is 0.4547\n",
      "time costed = 98.63661s \n",
      "\n",
      "***** Running training epoch 6 *****\n",
      "Epoch 0006 | Step 0062/1249 | Loss 0.7860 | Time 4.7879\n",
      "Epoch 0006 | Step 0124/1249 | Loss 0.7861 | Time 9.4959\n",
      "Epoch 0006 | Step 0186/1249 | Loss 0.7912 | Time 14.1685\n",
      "Epoch 0006 | Step 0248/1249 | Loss 0.7892 | Time 18.8878\n",
      "Epoch 0006 | Step 0310/1249 | Loss 0.7854 | Time 23.6179\n",
      "Epoch 0006 | Step 0372/1249 | Loss 0.7826 | Time 28.3194\n",
      "Epoch 0006 | Step 0434/1249 | Loss 0.7826 | Time 33.0736\n",
      "Epoch 0006 | Step 0496/1249 | Loss 0.7869 | Time 37.7897\n",
      "Epoch 0006 | Step 0558/1249 | Loss 0.7865 | Time 42.5539\n",
      "Epoch 0006 | Step 0620/1249 | Loss 0.7855 | Time 47.3139\n",
      "Epoch 0006 | Step 0682/1249 | Loss 0.7877 | Time 52.1692\n",
      "Epoch 0006 | Step 0744/1249 | Loss 0.7859 | Time 56.8980\n",
      "Epoch 0006 | Step 0806/1249 | Loss 0.7850 | Time 61.6572\n",
      "Epoch 0006 | Step 0868/1249 | Loss 0.7836 | Time 66.9879\n",
      "Epoch 0006 | Step 0930/1249 | Loss 0.7845 | Time 71.7840\n",
      "Epoch 0006 | Step 0992/1249 | Loss 0.7838 | Time 76.5066\n",
      "Epoch 0006 | Step 1054/1249 | Loss 0.7828 | Time 81.2080\n",
      "Epoch 0006 | Step 1116/1249 | Loss 0.7824 | Time 85.9639\n",
      "Epoch 0006 | Step 1178/1249 | Loss 0.7837 | Time 90.6885\n",
      "Epoch 0006 | Step 1240/1249 | Loss 0.7831 | Time 95.3847\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7072    0.8170    0.7581       470\n",
      "           1     0.5529    0.6267    0.5875       150\n",
      "           2     0.3226    0.2500    0.2817        40\n",
      "           3     0.5000    0.3063    0.3799       111\n",
      "           4     0.5583    0.5583    0.5583       163\n",
      "           5     0.2222    0.1818    0.2000        22\n",
      "           6     0.4741    0.3595    0.4089       153\n",
      "\n",
      "    accuracy                         0.6060      1109\n",
      "   macro avg     0.4768    0.4428    0.4535      1109\n",
      "weighted avg     0.5881    0.6060    0.5914      1109\n",
      "\n",
      "current macro_f1 is 0.4535, best macro_f1 is 0.4547\n",
      "time costed = 98.29607s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练和验证评估\n",
    "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载最优模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7341    0.8639    0.7937      1256\n",
      "           1     0.5213    0.5658    0.5427       281\n",
      "           2     0.2632    0.2000    0.2273        50\n",
      "           3     0.4907    0.2548    0.3354       208\n",
      "           4     0.5537    0.5771    0.5652       402\n",
      "           5     0.3636    0.1176    0.1778        68\n",
      "           6     0.5417    0.3768    0.4444       345\n",
      "\n",
      "    accuracy                         0.6425      2610\n",
      "   macro avg     0.4955    0.4223    0.4409      2610\n",
      "weighted avg     0.6199    0.6425    0.6219      2610\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e84238b33144f58c757b779faa987a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▂▄▄▄▄▄▄▅▅▅▅▅▅▅▇▇▇▇▇▇███████</td></tr><tr><td>lr</td><td>▂▆███████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▂▄▁▂▄▅▁▂▅▆█▅▄▁▁▃▃▃▃▃▃█▂▃▂▃▁▂▁▂▂▁▂▁▂▃▃▁▂</td></tr><tr><td>valid_acc</td><td>▄▆█▃▁▄</td></tr><tr><td>valid_loss</td><td>█▄▁▄▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>6</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.81535</td></tr><tr><td>valid_acc</td><td>0.60595</td></tr><tr><td>valid_loss</td><td>1.11797</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">classic-voice-11</strong>: <a href=\"https://wandb.ai/qftie/meld-emo-cls/runs/3qu2w28o\" target=\"_blank\">https://wandb.ai/qftie/meld-emo-cls/runs/3qu2w28o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220425_095055-3qu2w28o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载最优权重对测试集测试\n",
    "model.load_state_dict(torch.load(\"pytorch_model.bin\"))\n",
    "pred_test = evaluate(model, test_loader, DEVICE)\n",
    "# print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(test_label, pred_test)))\n",
    "# print(classification_report(test_label, pred_test, digits=4))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = 'output/ch-roberta-dorwardUt'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# torch.save(model.state_dict(), output_dir+\"/pytorch_model.bin\")\n",
    "# # torch.save(model, \"pytorch_model_whole.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49e5bd26eb5fb0a8ffb649f62262c127e261ba51230dc2578599ab5938abf7ca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('torch17py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
